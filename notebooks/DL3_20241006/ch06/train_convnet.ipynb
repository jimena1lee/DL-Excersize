{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n",
            "/\n",
            "train loss:2.2991952538629574\n",
            "=== epoch:1, train acc:0.102, test acc:0.094 ===\n",
            "train loss:2.2967139117201465\n",
            "train loss:2.295829584040485\n",
            "train loss:2.2875254823961746\n",
            "train loss:2.2817270920881665\n",
            "train loss:2.278148877959577\n",
            "train loss:2.2645474293304453\n",
            "train loss:2.24589412691143\n",
            "train loss:2.219643628650271\n",
            "train loss:2.190565953009974\n",
            "train loss:2.1826775453693448\n",
            "train loss:2.1513997175048587\n",
            "train loss:2.111506094070638\n",
            "train loss:2.0689059851714693\n",
            "train loss:1.9650404174338598\n",
            "train loss:1.9636815218324828\n",
            "train loss:1.9067503688628407\n",
            "train loss:1.8192140597139201\n",
            "train loss:1.7026128983057678\n",
            "train loss:1.6649012168454471\n",
            "train loss:1.6323679728025329\n",
            "train loss:1.5606327981584647\n",
            "train loss:1.413100692803014\n",
            "train loss:1.4020509732249864\n",
            "train loss:1.1954695121912584\n",
            "train loss:1.3169045594095194\n",
            "train loss:1.133918231619954\n",
            "train loss:1.016943548406119\n",
            "train loss:0.9432025468657348\n",
            "train loss:0.8711657835637648\n",
            "train loss:0.8524709852216791\n",
            "train loss:0.8763241368183617\n",
            "train loss:0.8377936776266672\n",
            "train loss:0.7811764711580215\n",
            "train loss:0.6906539637175036\n",
            "train loss:0.6766849850502338\n",
            "train loss:0.6521677444404488\n",
            "train loss:0.7037631234591305\n",
            "train loss:0.6354563155044497\n",
            "train loss:0.5813283265904158\n",
            "train loss:0.644344128927034\n",
            "train loss:0.6341066177562422\n",
            "train loss:0.5791842510931218\n",
            "train loss:0.578047490185152\n",
            "train loss:0.503685838154891\n",
            "train loss:0.6810324275896505\n",
            "train loss:0.6410547127652864\n",
            "train loss:0.47470318477780593\n",
            "train loss:0.5613892030350724\n",
            "train loss:0.6760593258010553\n",
            "train loss:0.48826978667620663\n",
            "train loss:0.7073511729335761\n",
            "train loss:0.5480873097480804\n",
            "train loss:0.5754771492350488\n",
            "train loss:0.5458020285889664\n",
            "train loss:0.573977488056825\n",
            "train loss:0.6976149248369041\n",
            "train loss:0.4181860697683877\n",
            "train loss:0.3156844773023844\n",
            "train loss:0.47273087504962896\n",
            "train loss:0.562227819029934\n",
            "train loss:0.3803848948365845\n",
            "train loss:0.4066812858889001\n",
            "train loss:0.44477839180672873\n",
            "train loss:0.41471557108612905\n",
            "train loss:0.4222362580492497\n",
            "train loss:0.3312040007549396\n",
            "train loss:0.6266079977727261\n",
            "train loss:0.31506426186775943\n",
            "train loss:0.5144060273725325\n",
            "train loss:0.4990512040782569\n",
            "train loss:0.4506922008588807\n",
            "train loss:0.5432114280267847\n",
            "train loss:0.4020866487294439\n",
            "train loss:0.3882310404007878\n",
            "train loss:0.3648261803638515\n",
            "train loss:0.35896083121782163\n",
            "train loss:0.5681238831979615\n",
            "train loss:0.3315281523730018\n",
            "train loss:0.32765438945695363\n",
            "train loss:0.3993413436499689\n",
            "train loss:0.37134483473040414\n",
            "train loss:0.5188460762077967\n",
            "train loss:0.2993241839558288\n",
            "train loss:0.3180347916385014\n",
            "train loss:0.2966410923513683\n",
            "train loss:0.3391591062354765\n",
            "train loss:0.44083855341019107\n",
            "train loss:0.5641977107244189\n",
            "train loss:0.3828706336946899\n",
            "train loss:0.40060479812383565\n",
            "train loss:0.39151919486468073\n",
            "train loss:0.4527983062340553\n",
            "train loss:0.4589068057315693\n",
            "train loss:0.6044946486962778\n",
            "train loss:0.3379058326526671\n",
            "train loss:0.4089011937481164\n",
            "train loss:0.3678849275981907\n",
            "train loss:0.33113031647889263\n",
            "train loss:0.38618140557371605\n",
            "train loss:0.24295622506109296\n",
            "=== epoch:2, train acc:0.877, test acc:0.865 ===\n",
            "train loss:0.2590185594741058\n",
            "train loss:0.2048508982027691\n",
            "train loss:0.3517860336620393\n",
            "train loss:0.2601756946051442\n",
            "train loss:0.4002889819169511\n",
            "train loss:0.3682256860167389\n",
            "train loss:0.30982648020105086\n",
            "train loss:0.44869254812456627\n",
            "train loss:0.3098426164753931\n",
            "train loss:0.32239486625173236\n",
            "train loss:0.4604711328195305\n",
            "train loss:0.296423067410719\n",
            "train loss:0.2014349925295793\n",
            "train loss:0.32222685082253344\n",
            "train loss:0.38397946235628266\n",
            "train loss:0.2802003811864482\n",
            "train loss:0.3432563352628906\n",
            "train loss:0.45810810694349974\n",
            "train loss:0.3575592215504294\n",
            "train loss:0.2565828196562979\n",
            "train loss:0.3648525798591399\n",
            "train loss:0.3074691041441359\n",
            "train loss:0.31650619363277116\n",
            "train loss:0.22210443220115764\n",
            "train loss:0.3650407474379049\n",
            "train loss:0.4329320524628056\n",
            "train loss:0.2691752202371228\n",
            "train loss:0.17264101986855807\n",
            "train loss:0.3306621436803717\n",
            "train loss:0.24732602939813966\n",
            "train loss:0.3085515057968232\n",
            "train loss:0.29694405513203886\n",
            "train loss:0.30361573351589505\n",
            "train loss:0.2994493988352026\n",
            "train loss:0.24047646375414447\n",
            "train loss:0.3242448325062391\n",
            "train loss:0.43331628971345104\n",
            "train loss:0.3893164266841729\n",
            "train loss:0.42076906749066445\n",
            "train loss:0.30320420382904184\n",
            "train loss:0.2820214017957728\n",
            "train loss:0.2903354192127388\n",
            "train loss:0.24565684191527043\n",
            "train loss:0.3662246022341031\n",
            "train loss:0.18598668845281618\n",
            "train loss:0.3324688167290175\n",
            "train loss:0.6226461788583744\n",
            "train loss:0.36006414040566115\n",
            "train loss:0.3079225191293232\n",
            "train loss:0.5326096748331357\n",
            "train loss:0.30327871954722374\n",
            "train loss:0.26908178924953585\n",
            "train loss:0.2781095960950651\n",
            "train loss:0.2764846032013836\n",
            "train loss:0.27531204568808987\n",
            "train loss:0.34522075869818075\n",
            "train loss:0.2717586106235242\n",
            "train loss:0.25860520213759197\n",
            "train loss:0.23912981545603876\n",
            "train loss:0.3050337730619345\n",
            "train loss:0.22234006750847896\n",
            "train loss:0.2799055444239743\n",
            "train loss:0.24142169759434753\n",
            "train loss:0.41292937561464854\n",
            "train loss:0.4357422296735028\n",
            "train loss:0.2258808506183016\n",
            "train loss:0.23280732293432504\n",
            "train loss:0.3207625248955345\n",
            "train loss:0.2404027306928295\n",
            "train loss:0.4564093243344726\n",
            "train loss:0.35436207119976404\n",
            "train loss:0.24987693026492774\n",
            "train loss:0.3129861760997544\n",
            "train loss:0.17282166390766387\n",
            "train loss:0.24743742614321804\n",
            "train loss:0.15509853323091194\n",
            "train loss:0.2368401159577589\n",
            "train loss:0.3610117050223789\n",
            "train loss:0.2253045335346985\n",
            "train loss:0.21784614385486406\n",
            "train loss:0.23948799699118484\n",
            "train loss:0.2779840935835685\n",
            "train loss:0.20985508057499827\n",
            "train loss:0.319905720450207\n",
            "train loss:0.270318029971449\n",
            "train loss:0.13022209661932366\n",
            "train loss:0.39307213080694337\n",
            "train loss:0.20193264468669228\n",
            "train loss:0.33509280208614806\n",
            "train loss:0.15555930819044753\n",
            "train loss:0.3913022457405089\n",
            "train loss:0.2857122301219317\n",
            "train loss:0.3077497416297624\n",
            "train loss:0.26075724086914015\n",
            "train loss:0.19181317297244338\n",
            "train loss:0.2650333782876576\n",
            "train loss:0.3668360544629421\n",
            "train loss:0.24238054128541528\n",
            "train loss:0.2869650642175471\n",
            "train loss:0.23192297327332403\n",
            "=== epoch:3, train acc:0.913, test acc:0.904 ===\n",
            "train loss:0.2879012673591193\n",
            "train loss:0.24539430059437378\n",
            "train loss:0.3601353727135904\n",
            "train loss:0.09307403986045358\n",
            "train loss:0.23412512280400657\n",
            "train loss:0.26539805901752617\n",
            "train loss:0.29632991758685134\n",
            "train loss:0.17907425051866518\n",
            "train loss:0.1875584626637928\n",
            "train loss:0.3303366937149451\n",
            "train loss:0.20410865241016893\n",
            "train loss:0.21709542502681928\n",
            "train loss:0.15720645158261695\n",
            "train loss:0.22654268329665053\n",
            "train loss:0.22736415387059838\n",
            "train loss:0.2512819143731461\n",
            "train loss:0.20054938527285326\n",
            "train loss:0.21060361392678545\n",
            "train loss:0.34968397467567514\n",
            "train loss:0.20015519975768822\n",
            "train loss:0.24257244981746096\n",
            "train loss:0.22242825331292213\n",
            "train loss:0.34587414992634274\n",
            "train loss:0.17824766216759727\n",
            "train loss:0.1856624139789188\n",
            "train loss:0.29661937613391837\n",
            "train loss:0.2951203369038577\n",
            "train loss:0.21997895060599137\n",
            "train loss:0.30204237150221125\n",
            "train loss:0.1728871913125246\n",
            "train loss:0.23059135329583327\n",
            "train loss:0.21761835102752955\n",
            "train loss:0.31703922806949814\n",
            "train loss:0.14356361070967846\n",
            "train loss:0.24406299270558343\n",
            "train loss:0.18750223685479012\n",
            "train loss:0.28277547572416084\n",
            "train loss:0.1431177429924478\n",
            "train loss:0.2519213997714733\n",
            "train loss:0.10483052413936697\n",
            "train loss:0.1321096279744083\n",
            "train loss:0.4033451807681546\n",
            "train loss:0.10257063809402228\n",
            "train loss:0.21392321028267058\n",
            "train loss:0.3748164962039427\n",
            "train loss:0.2418441953475014\n",
            "train loss:0.29254493820788685\n",
            "train loss:0.22259383350409348\n",
            "train loss:0.28309961068629874\n",
            "train loss:0.3367326384363912\n",
            "train loss:0.4152371991065724\n",
            "train loss:0.35925552177902026\n",
            "train loss:0.15271924518966795\n",
            "train loss:0.162912828776577\n",
            "train loss:0.23768026134392117\n",
            "train loss:0.20625772489798827\n",
            "train loss:0.16346389731909092\n",
            "train loss:0.16411399627526702\n",
            "train loss:0.21103782203152058\n",
            "train loss:0.13317543918854124\n",
            "train loss:0.254328769206656\n",
            "train loss:0.2332822431829571\n",
            "train loss:0.21468226980047514\n",
            "train loss:0.2239395571086867\n",
            "train loss:0.35828930888312266\n",
            "train loss:0.32551254982749456\n",
            "train loss:0.21568741240653602\n",
            "train loss:0.14221143714683004\n",
            "train loss:0.17939414401241766\n",
            "train loss:0.2774554323580074\n",
            "train loss:0.18468637499468657\n",
            "train loss:0.19592216148740774\n",
            "train loss:0.11785299461246397\n",
            "train loss:0.09449552231189577\n",
            "train loss:0.09987951983091227\n",
            "train loss:0.14435731951194508\n",
            "train loss:0.3212750623401233\n",
            "train loss:0.22340568332295746\n",
            "train loss:0.3399111764251361\n",
            "train loss:0.24566484507434244\n",
            "train loss:0.4865158357196829\n",
            "train loss:0.3293214037650328\n",
            "train loss:0.11784368415326561\n",
            "train loss:0.24166318725259028\n",
            "train loss:0.13071868753864854\n",
            "train loss:0.4023460503693038\n",
            "train loss:0.19543633145902548\n",
            "train loss:0.3064951778442729\n",
            "train loss:0.23383952251351175\n",
            "train loss:0.2683497523094001\n",
            "train loss:0.1763913094204952\n",
            "train loss:0.18130957253341073\n",
            "train loss:0.21557576446618762\n",
            "train loss:0.2519937347857105\n",
            "train loss:0.2369101780785212\n",
            "train loss:0.1884394534051705\n",
            "train loss:0.2882330908596655\n",
            "train loss:0.218579461458067\n",
            "train loss:0.16134056106647487\n",
            "train loss:0.2776107686925928\n",
            "=== epoch:4, train acc:0.933, test acc:0.917 ===\n",
            "train loss:0.20484698372640264\n",
            "train loss:0.39457359577309226\n",
            "train loss:0.17799569236667956\n",
            "train loss:0.29597757030394684\n",
            "train loss:0.21319201449674544\n",
            "train loss:0.20812035951152144\n",
            "train loss:0.12296574692928171\n",
            "train loss:0.12163717339668238\n",
            "train loss:0.13097400101739745\n",
            "train loss:0.21238580099380816\n",
            "train loss:0.2831611353094548\n",
            "train loss:0.19772706319008992\n",
            "train loss:0.17472917116993394\n",
            "train loss:0.1906558588619227\n",
            "train loss:0.14570623353968848\n",
            "train loss:0.1675004452687492\n",
            "train loss:0.19727005786665472\n",
            "train loss:0.14653094838436445\n",
            "train loss:0.193066582581077\n",
            "train loss:0.16381850693000985\n",
            "train loss:0.21970204318251965\n",
            "train loss:0.2778554092827267\n",
            "train loss:0.21540441889705048\n",
            "train loss:0.11257960214945284\n",
            "train loss:0.10442514594515658\n",
            "train loss:0.16933188259892584\n",
            "train loss:0.1339981848110897\n",
            "train loss:0.18927856695977138\n",
            "train loss:0.24038909085347757\n",
            "train loss:0.19644565820423487\n",
            "train loss:0.13476964180432816\n",
            "train loss:0.19739201282104843\n",
            "train loss:0.18844741855750013\n",
            "train loss:0.2676473872997803\n",
            "train loss:0.1766303527099431\n",
            "train loss:0.22185975924251278\n",
            "train loss:0.1336511295194798\n",
            "train loss:0.1867525932591836\n",
            "train loss:0.15091514267241135\n",
            "train loss:0.1685666866052748\n",
            "train loss:0.2225751345761126\n",
            "train loss:0.1807867872011163\n",
            "train loss:0.16046718130413523\n",
            "train loss:0.26457402637525135\n",
            "train loss:0.16224253652886347\n",
            "train loss:0.19229817452534473\n",
            "train loss:0.13432774661682756\n",
            "train loss:0.24561289836395186\n",
            "train loss:0.15603922526411862\n",
            "train loss:0.18884382554712498\n",
            "train loss:0.1727221371390955\n",
            "train loss:0.2836768203861386\n",
            "train loss:0.15504043018888755\n",
            "train loss:0.0884151971037043\n",
            "train loss:0.10041161439093386\n",
            "train loss:0.10292426100579638\n",
            "train loss:0.08247525991001269\n",
            "train loss:0.26028390384014033\n",
            "train loss:0.17796566559805957\n",
            "train loss:0.14438709514306716\n",
            "train loss:0.14688124176303405\n",
            "train loss:0.24545465666117447\n",
            "train loss:0.25009340579321476\n",
            "train loss:0.25950919425745594\n",
            "train loss:0.13337234455662336\n",
            "train loss:0.07337001605407979\n",
            "train loss:0.17387090876229117\n",
            "train loss:0.19801975785147224\n",
            "train loss:0.19018640623423522\n",
            "train loss:0.1724553290371408\n",
            "train loss:0.0584877895772293\n",
            "train loss:0.1586551705608803\n",
            "train loss:0.19038136370991604\n",
            "train loss:0.12750369907387835\n",
            "train loss:0.07467756704364036\n",
            "train loss:0.1498171927637909\n",
            "train loss:0.20063405794075834\n",
            "train loss:0.1351494193876278\n",
            "train loss:0.10244925631733065\n",
            "train loss:0.11380007570347343\n",
            "train loss:0.15339640080863395\n",
            "train loss:0.2598437360152336\n",
            "train loss:0.19479058405101285\n",
            "train loss:0.10962494928433184\n",
            "train loss:0.19810758491916935\n",
            "train loss:0.18471046405223324\n",
            "train loss:0.16433054451280124\n",
            "train loss:0.2037781637131859\n",
            "train loss:0.09450221745235736\n",
            "train loss:0.27019446666259195\n",
            "train loss:0.14412883170847046\n",
            "train loss:0.1913246004896345\n",
            "train loss:0.1747638986775582\n",
            "train loss:0.22193952205115597\n",
            "train loss:0.21506131404472165\n",
            "train loss:0.1356212993425221\n",
            "train loss:0.17903324985024355\n",
            "train loss:0.11103293837638895\n",
            "train loss:0.17224678157985895\n",
            "train loss:0.14534446557584538\n",
            "=== epoch:5, train acc:0.951, test acc:0.936 ===\n",
            "train loss:0.09734689757345666\n",
            "train loss:0.09040954895060542\n",
            "train loss:0.13320599515485942\n",
            "train loss:0.10099124420119467\n",
            "train loss:0.09376582933058707\n",
            "train loss:0.14746260185406995\n",
            "train loss:0.13737953330141817\n",
            "train loss:0.10944459867122586\n",
            "train loss:0.18351597393805683\n",
            "train loss:0.17315062859434555\n",
            "train loss:0.2521296535254976\n",
            "train loss:0.1744210181883894\n",
            "train loss:0.09838095056658838\n",
            "train loss:0.09571352037737442\n",
            "train loss:0.19278427454768018\n",
            "train loss:0.07775126970407298\n",
            "train loss:0.08633471715519239\n",
            "train loss:0.1545387487384198\n",
            "train loss:0.07069209422265134\n",
            "train loss:0.06199906534698571\n",
            "train loss:0.1680855717998118\n",
            "train loss:0.2618090544018084\n",
            "train loss:0.13236686280999788\n",
            "train loss:0.1166310870399147\n",
            "train loss:0.09329210515675519\n",
            "train loss:0.12770779904361784\n",
            "train loss:0.19045308902751792\n",
            "train loss:0.10745930134373774\n",
            "train loss:0.1904563675582962\n",
            "train loss:0.1778183922463687\n",
            "train loss:0.19212878125674135\n",
            "train loss:0.09290725519502509\n",
            "train loss:0.12037605863982917\n",
            "train loss:0.13529672029137824\n",
            "train loss:0.18671536184736165\n",
            "train loss:0.10482194217795314\n",
            "train loss:0.15170865720010443\n",
            "train loss:0.24195827513867474\n",
            "train loss:0.30154701829549074\n",
            "train loss:0.07831513028926958\n",
            "train loss:0.11772964952720737\n",
            "train loss:0.08273129884103296\n",
            "train loss:0.18288694532232552\n",
            "train loss:0.0655952596736289\n",
            "train loss:0.07707867907282036\n",
            "train loss:0.12664041466245016\n",
            "train loss:0.06758398497231821\n",
            "train loss:0.23133777968263008\n",
            "train loss:0.1314157710397906\n",
            "train loss:0.10909234040466616\n",
            "train loss:0.2285432447639526\n",
            "train loss:0.2002789448868411\n",
            "train loss:0.1496599949370354\n",
            "train loss:0.07523103569466859\n",
            "train loss:0.13977192024455024\n",
            "train loss:0.13634709664313405\n",
            "train loss:0.11670831389590976\n",
            "train loss:0.10580089203418266\n",
            "train loss:0.21005468825564677\n",
            "train loss:0.1528620770477592\n",
            "train loss:0.1312870961154727\n",
            "train loss:0.09626909586474232\n",
            "train loss:0.3826682852738619\n",
            "train loss:0.06177583739394171\n",
            "train loss:0.09671381505244465\n",
            "train loss:0.1289355877004626\n",
            "train loss:0.07745995541331933\n",
            "train loss:0.0839829413356253\n",
            "train loss:0.10873825094865289\n",
            "train loss:0.19736780137093451\n",
            "train loss:0.11253455435932089\n",
            "train loss:0.18470179023390323\n",
            "train loss:0.18979018338734008\n",
            "train loss:0.18068862451643053\n",
            "train loss:0.050039319037618754\n",
            "train loss:0.15412707025361685\n",
            "train loss:0.2105891197384363\n",
            "train loss:0.15904405976413524\n",
            "train loss:0.06579963338823218\n",
            "train loss:0.08613758463624394\n",
            "train loss:0.15246372450745846\n",
            "train loss:0.10776451986882707\n",
            "train loss:0.057677152462412916\n",
            "train loss:0.18295004656183494\n",
            "train loss:0.03999962203211905\n",
            "train loss:0.055510443374061406\n",
            "train loss:0.10587238090435781\n",
            "train loss:0.1786363939369335\n",
            "train loss:0.19397023056917598\n",
            "train loss:0.0638374972600479\n",
            "train loss:0.10640171938541144\n",
            "train loss:0.08509927864678027\n",
            "train loss:0.11874479639167208\n",
            "train loss:0.15050620530548733\n",
            "train loss:0.042099413930956404\n",
            "train loss:0.0545219534211789\n",
            "train loss:0.06667513625287794\n",
            "train loss:0.08490219376892312\n",
            "train loss:0.06409274112926129\n",
            "train loss:0.16185565674914845\n",
            "=== epoch:6, train acc:0.961, test acc:0.94 ===\n",
            "train loss:0.07683092330943823\n",
            "train loss:0.10253543752657485\n",
            "train loss:0.08076798796499311\n",
            "train loss:0.112947977364191\n",
            "train loss:0.05416810647399513\n",
            "train loss:0.16679701199520444\n",
            "train loss:0.08390684786653796\n",
            "train loss:0.07171775073467085\n",
            "train loss:0.20150109706703398\n",
            "train loss:0.1048084102945458\n",
            "train loss:0.0808668143660362\n",
            "train loss:0.0651021803515966\n",
            "train loss:0.057353966982729494\n",
            "train loss:0.08190618274152539\n",
            "train loss:0.10291555644167387\n",
            "train loss:0.0707661999567368\n",
            "train loss:0.13336394986336386\n",
            "train loss:0.09562132865015915\n",
            "train loss:0.14211823610168012\n",
            "train loss:0.08662897902409618\n",
            "train loss:0.07982109711267842\n",
            "train loss:0.08189306419873262\n",
            "train loss:0.1426373939060984\n",
            "train loss:0.13853034370736858\n",
            "train loss:0.07875268094370194\n",
            "train loss:0.060139825566950585\n",
            "train loss:0.16718780140766512\n",
            "train loss:0.08438851918970147\n",
            "train loss:0.09611427661779093\n",
            "train loss:0.1384683366353261\n",
            "train loss:0.08656465847870201\n",
            "train loss:0.0609828017937421\n",
            "train loss:0.10039608690992902\n",
            "train loss:0.07012478693232738\n",
            "train loss:0.11517518919089655\n",
            "train loss:0.13174849078550063\n",
            "train loss:0.11429075104140324\n",
            "train loss:0.11135127464920566\n",
            "train loss:0.07518291263805593\n",
            "train loss:0.14851349960986993\n",
            "train loss:0.16543601692180812\n",
            "train loss:0.05630197755600943\n",
            "train loss:0.1249468765181387\n",
            "train loss:0.08058580011713498\n",
            "train loss:0.09317038719914264\n",
            "train loss:0.034518880998756545\n",
            "train loss:0.20117768709309602\n",
            "train loss:0.19719092508921932\n",
            "train loss:0.14832521298021278\n",
            "train loss:0.05889881256824711\n",
            "train loss:0.10029957304597045\n",
            "train loss:0.08944225479836466\n",
            "train loss:0.08425815080857961\n",
            "train loss:0.16285238471930127\n",
            "train loss:0.042186315099545464\n",
            "train loss:0.09651632332434078\n",
            "train loss:0.11667506524153681\n",
            "train loss:0.09260357340820617\n",
            "train loss:0.1265810712556702\n",
            "train loss:0.09613061534632891\n",
            "train loss:0.14054168356342533\n",
            "train loss:0.09621057463666208\n",
            "train loss:0.08664876721338015\n",
            "train loss:0.06992687466001124\n",
            "train loss:0.12159480826001547\n",
            "train loss:0.10578892390099456\n",
            "train loss:0.06020044337713728\n",
            "train loss:0.14208005809673716\n",
            "train loss:0.09809192994400968\n",
            "train loss:0.06481198920238194\n",
            "train loss:0.059673573778101445\n",
            "train loss:0.1066748355212939\n",
            "train loss:0.13409011330022216\n",
            "train loss:0.17521478276698577\n",
            "train loss:0.1542938757932292\n",
            "train loss:0.04795724959630317\n",
            "train loss:0.11546217887728379\n",
            "train loss:0.1143724085046287\n",
            "train loss:0.18973696234476278\n",
            "train loss:0.07915997105826897\n",
            "train loss:0.029154938932290094\n",
            "train loss:0.06196174242520797\n",
            "train loss:0.05147755961311283\n",
            "train loss:0.07170358976509356\n",
            "train loss:0.04218358886367017\n",
            "train loss:0.10029173746587572\n",
            "train loss:0.047831167490886534\n",
            "train loss:0.20643731481442884\n",
            "train loss:0.022074874945518616\n",
            "train loss:0.1801637640128996\n",
            "train loss:0.049960258393689304\n",
            "train loss:0.08741278845620347\n",
            "train loss:0.08792537322572645\n",
            "train loss:0.12121181866019073\n",
            "train loss:0.13885780114622426\n",
            "train loss:0.1277871782279407\n",
            "train loss:0.09338139316189603\n",
            "train loss:0.15148540235904845\n",
            "train loss:0.038635533942346596\n",
            "train loss:0.08656353841100155\n",
            "=== epoch:7, train acc:0.962, test acc:0.957 ===\n",
            "train loss:0.04551523747701397\n",
            "train loss:0.09745241720753756\n",
            "train loss:0.059163331502681264\n",
            "train loss:0.05149960703425527\n",
            "train loss:0.078385353620771\n",
            "train loss:0.20072361242544898\n",
            "train loss:0.1802557805998806\n",
            "train loss:0.06712304214741222\n",
            "train loss:0.07792187044772472\n",
            "train loss:0.17506572536317733\n",
            "train loss:0.09424119255501112\n",
            "train loss:0.11069299336164465\n",
            "train loss:0.09417950208608654\n",
            "train loss:0.03017817076135408\n",
            "train loss:0.06834457796978652\n",
            "train loss:0.07968009451327147\n",
            "train loss:0.040590151340743404\n",
            "train loss:0.06574038888648764\n",
            "train loss:0.05461803503407221\n",
            "train loss:0.03835179068488509\n",
            "train loss:0.07165523185499675\n",
            "train loss:0.12821764059106341\n",
            "train loss:0.12770764555438074\n",
            "train loss:0.0909216277022257\n",
            "train loss:0.051057088484981715\n",
            "train loss:0.08127965354187298\n",
            "train loss:0.05768542068203141\n",
            "train loss:0.07530752664027267\n",
            "train loss:0.11014097335670396\n",
            "train loss:0.049121280318961444\n",
            "train loss:0.09259705253646916\n",
            "train loss:0.026945896065291065\n",
            "train loss:0.04891415219188417\n",
            "train loss:0.117663758051994\n",
            "train loss:0.05957412677354061\n",
            "train loss:0.0526805629894749\n",
            "train loss:0.1415443359395841\n",
            "train loss:0.04303525463432892\n",
            "train loss:0.09225107573192753\n",
            "train loss:0.10608106783687225\n",
            "train loss:0.08491684969468945\n",
            "train loss:0.11754119745538726\n",
            "train loss:0.10121915373233978\n",
            "train loss:0.031323054000100076\n",
            "train loss:0.10845166141705208\n",
            "train loss:0.05120636349788628\n",
            "train loss:0.138364378277752\n",
            "train loss:0.03659013845642086\n",
            "train loss:0.058451923271940806\n",
            "train loss:0.03665419911777445\n",
            "train loss:0.03906387659779629\n",
            "train loss:0.08097134926977839\n",
            "train loss:0.05535178902860852\n",
            "train loss:0.2398062032446558\n",
            "train loss:0.0466483907340685\n",
            "train loss:0.06566451710797802\n",
            "train loss:0.05128045413118018\n",
            "train loss:0.028594951308592475\n",
            "train loss:0.05613743804927247\n",
            "train loss:0.09946898671887164\n",
            "train loss:0.06154566683969401\n",
            "train loss:0.14811476202829996\n",
            "train loss:0.08286255351849858\n",
            "train loss:0.06921044381393732\n",
            "train loss:0.11350769515926355\n",
            "train loss:0.0956836511515646\n",
            "train loss:0.03564500664844331\n",
            "train loss:0.07799912548831639\n",
            "train loss:0.059744586560122076\n",
            "train loss:0.07652867972023128\n",
            "train loss:0.06366347619101505\n",
            "train loss:0.04174922161769801\n",
            "train loss:0.051154161022429936\n",
            "train loss:0.03912311892733284\n",
            "train loss:0.15530795746171946\n",
            "train loss:0.12362437001687585\n",
            "train loss:0.12680644040470684\n",
            "train loss:0.06540990655244111\n",
            "train loss:0.08661981029534799\n",
            "train loss:0.06225491421040703\n",
            "train loss:0.05635381989211099\n",
            "train loss:0.12170980722440236\n",
            "train loss:0.07426541341224896\n",
            "train loss:0.05638257111391875\n",
            "train loss:0.11909368215806174\n",
            "train loss:0.07376705132553668\n",
            "train loss:0.04914841931079926\n",
            "train loss:0.059941376552187384\n",
            "train loss:0.10155066166550426\n",
            "train loss:0.12154255242531206\n",
            "train loss:0.0324560600903943\n",
            "train loss:0.11210892519002956\n",
            "train loss:0.07232619719600833\n",
            "train loss:0.11277567545003617\n",
            "train loss:0.06467526119683022\n",
            "train loss:0.04831314447268764\n",
            "train loss:0.061727229672611524\n",
            "train loss:0.10960987324981582\n",
            "train loss:0.030167276898044548\n",
            "train loss:0.05664723202059804\n",
            "=== epoch:8, train acc:0.97, test acc:0.957 ===\n",
            "train loss:0.049821023030422566\n",
            "train loss:0.08087542387853854\n",
            "train loss:0.08651556024510249\n",
            "train loss:0.07410733443953955\n",
            "train loss:0.05756868495824273\n",
            "train loss:0.05518903046778543\n",
            "train loss:0.06498373655772305\n",
            "train loss:0.10714895730296764\n",
            "train loss:0.04964688838447048\n",
            "train loss:0.10784906234953492\n",
            "train loss:0.054833754073126434\n",
            "train loss:0.10054670258431653\n",
            "train loss:0.11342038762412708\n",
            "train loss:0.0717709784746666\n",
            "train loss:0.07832146475897112\n",
            "train loss:0.09041774310639514\n",
            "train loss:0.04652697359235424\n",
            "train loss:0.037192240229904375\n",
            "train loss:0.059760692887274\n",
            "train loss:0.07521607133231896\n",
            "train loss:0.07945432071072658\n",
            "train loss:0.061862821001984786\n",
            "train loss:0.03236521995024682\n",
            "train loss:0.05497135842190356\n",
            "train loss:0.07441582399618972\n",
            "train loss:0.03893945031189235\n",
            "train loss:0.07022088409759428\n",
            "train loss:0.048826255758350576\n",
            "train loss:0.11086526917343355\n",
            "train loss:0.03854253702997063\n",
            "train loss:0.08116506928663378\n",
            "train loss:0.04038296664801384\n",
            "train loss:0.13950211851735694\n",
            "train loss:0.02712803312719204\n",
            "train loss:0.07739978406122736\n",
            "train loss:0.04864558175009437\n",
            "train loss:0.16051169978165242\n",
            "train loss:0.06924526252195037\n",
            "train loss:0.0466437056619935\n",
            "train loss:0.05086475618791608\n",
            "train loss:0.03785059687457817\n",
            "train loss:0.07184823122311641\n",
            "train loss:0.09349518801013396\n",
            "train loss:0.09673448439822675\n",
            "train loss:0.06427119766314922\n",
            "train loss:0.047520182900056024\n",
            "train loss:0.09044977401017393\n",
            "train loss:0.022040983013943825\n",
            "train loss:0.07866846751743281\n",
            "train loss:0.028578483460349108\n",
            "train loss:0.12688414522116495\n",
            "train loss:0.03081888077795422\n",
            "train loss:0.055184369492383326\n",
            "train loss:0.055168151750046864\n",
            "train loss:0.02390155074399098\n",
            "train loss:0.04705441492363566\n",
            "train loss:0.044061436299775526\n",
            "train loss:0.11241169276336763\n",
            "train loss:0.04437334707693052\n",
            "train loss:0.054824539998900995\n",
            "train loss:0.1450512649363269\n",
            "train loss:0.13388788818820804\n",
            "train loss:0.07998544173414358\n",
            "train loss:0.026667805470119422\n",
            "train loss:0.04198760187425284\n",
            "train loss:0.08812272149574248\n",
            "train loss:0.02101981769476529\n",
            "train loss:0.09404445457332875\n",
            "train loss:0.16603475710731302\n",
            "train loss:0.08372491579438612\n",
            "train loss:0.07139813699940528\n",
            "train loss:0.04175423991595391\n",
            "train loss:0.11029680351866246\n",
            "train loss:0.05250454856079749\n",
            "train loss:0.0651522518034118\n",
            "train loss:0.09459529755894412\n",
            "train loss:0.07519726672302708\n",
            "train loss:0.02430773216333566\n",
            "train loss:0.04182544001409032\n",
            "train loss:0.04292961907075906\n",
            "train loss:0.05019556580467688\n",
            "train loss:0.06550737018102269\n",
            "train loss:0.04998760572288889\n",
            "train loss:0.07441563019574841\n",
            "train loss:0.019903659655098892\n",
            "train loss:0.0561728434903486\n",
            "train loss:0.04709664367337298\n",
            "train loss:0.026723068719562298\n",
            "train loss:0.050307710081715695\n",
            "train loss:0.039783675395936566\n",
            "train loss:0.03355534220180931\n",
            "train loss:0.10923974153173431\n",
            "train loss:0.08150368662244059\n",
            "train loss:0.07436958710217743\n",
            "train loss:0.06815370769580889\n",
            "train loss:0.0320515196129375\n",
            "train loss:0.11410408602876439\n",
            "train loss:0.03072651748843812\n",
            "train loss:0.0955951607887954\n",
            "train loss:0.10381268787959069\n",
            "=== epoch:9, train acc:0.976, test acc:0.967 ===\n",
            "train loss:0.04254885282477426\n",
            "train loss:0.05739590216255182\n",
            "train loss:0.038892176154141624\n",
            "train loss:0.05690977139244899\n",
            "train loss:0.06621996909059441\n",
            "train loss:0.026402734427300695\n",
            "train loss:0.031179588490272102\n",
            "train loss:0.06341903610104609\n",
            "train loss:0.0498207086593979\n",
            "train loss:0.03828078372256393\n",
            "train loss:0.03584801463087932\n",
            "train loss:0.020163442027668507\n",
            "train loss:0.02054920965855646\n",
            "train loss:0.030849027144417365\n",
            "train loss:0.0824662973303299\n",
            "train loss:0.027759991642070415\n",
            "train loss:0.06260343407166306\n",
            "train loss:0.05452272387152351\n",
            "train loss:0.0175398652845968\n",
            "train loss:0.08951078372219175\n",
            "train loss:0.01788413330130898\n",
            "train loss:0.056780748964153985\n",
            "train loss:0.19472415383861136\n",
            "train loss:0.12695493616194262\n",
            "train loss:0.06321771492296391\n",
            "train loss:0.047830573625406644\n",
            "train loss:0.035300594708370096\n",
            "train loss:0.054150814262132176\n",
            "train loss:0.09362061094720818\n",
            "train loss:0.06925767557970619\n",
            "train loss:0.2527938535427695\n",
            "train loss:0.08243277092179774\n",
            "train loss:0.16267979879911276\n",
            "train loss:0.0775285465609099\n",
            "train loss:0.054739526891194636\n",
            "train loss:0.04867724895996611\n",
            "train loss:0.030143171943577806\n",
            "train loss:0.08277721248615451\n",
            "train loss:0.06458295412988414\n",
            "train loss:0.04291480714048927\n",
            "train loss:0.04886355031614437\n",
            "train loss:0.05798035093086333\n",
            "train loss:0.0519394595326814\n",
            "train loss:0.06887266374007636\n",
            "train loss:0.07849995820536596\n",
            "train loss:0.07982242017749043\n",
            "train loss:0.17673818367557775\n",
            "train loss:0.08651061658863539\n",
            "train loss:0.035645556985438295\n",
            "train loss:0.04428091079309115\n",
            "train loss:0.09532882165172549\n",
            "train loss:0.06703871167763474\n",
            "train loss:0.06868980119789421\n",
            "train loss:0.06603372827029545\n",
            "train loss:0.047918593193506887\n",
            "train loss:0.04923428891995119\n",
            "train loss:0.04661601808076751\n",
            "train loss:0.08272414753318987\n",
            "train loss:0.06748044533349318\n",
            "train loss:0.05960193598914101\n",
            "train loss:0.07225384575760818\n",
            "train loss:0.054329454151036315\n",
            "train loss:0.06960452681594259\n",
            "train loss:0.05556021153375723\n",
            "train loss:0.042592528650794784\n",
            "train loss:0.11209114634640212\n",
            "train loss:0.043357072404939564\n",
            "train loss:0.05540924333138557\n",
            "train loss:0.044910775249191265\n",
            "train loss:0.03150829696236933\n",
            "train loss:0.03145705556513475\n",
            "train loss:0.09003620362180159\n",
            "train loss:0.11211786032862203\n",
            "train loss:0.09419008347619039\n",
            "train loss:0.06373370096724545\n",
            "train loss:0.03792602102915906\n",
            "train loss:0.05327822192499542\n",
            "train loss:0.027120381620159534\n",
            "train loss:0.0594799077837268\n",
            "train loss:0.03197143860557246\n",
            "train loss:0.03358406908742012\n",
            "train loss:0.09930861291635619\n",
            "train loss:0.025497750064543908\n",
            "train loss:0.03217640924858957\n",
            "train loss:0.13014140745430053\n",
            "train loss:0.06968126973523044\n",
            "train loss:0.030331721124457794\n",
            "train loss:0.09452777897361506\n",
            "train loss:0.055502985273972474\n",
            "train loss:0.05047666869057431\n",
            "train loss:0.02742067528922161\n",
            "train loss:0.11857534278610665\n",
            "train loss:0.043269054250606544\n",
            "train loss:0.041379983145904194\n",
            "train loss:0.03247943640260327\n",
            "train loss:0.03631272856215814\n",
            "train loss:0.05002577956587023\n",
            "train loss:0.07469485683679203\n",
            "train loss:0.05685954900726634\n",
            "train loss:0.03118634037277572\n",
            "=== epoch:10, train acc:0.973, test acc:0.964 ===\n",
            "train loss:0.053001634208039695\n",
            "train loss:0.07239116882710946\n",
            "train loss:0.038594386547694856\n",
            "train loss:0.040082270196278226\n",
            "train loss:0.03422744389035027\n",
            "train loss:0.03284400397230048\n",
            "train loss:0.10359275927560146\n",
            "train loss:0.035699773354511063\n",
            "train loss:0.04598189433827643\n",
            "train loss:0.027436566234376368\n",
            "train loss:0.02563202329870002\n",
            "train loss:0.09842295300549284\n",
            "train loss:0.07749009357415237\n",
            "train loss:0.07690260454004609\n",
            "train loss:0.023678654716089473\n",
            "train loss:0.02901865393343131\n",
            "train loss:0.030243892351180444\n",
            "train loss:0.10256656148470399\n",
            "train loss:0.027222471057891395\n",
            "train loss:0.04539514744568864\n",
            "train loss:0.032486246412140464\n",
            "train loss:0.07378782986806966\n",
            "train loss:0.03802297766849306\n",
            "train loss:0.04911888133913718\n",
            "train loss:0.028210536121576837\n",
            "train loss:0.02325720225676977\n",
            "train loss:0.14754290358043554\n",
            "train loss:0.07667445497392034\n",
            "train loss:0.10266140000760167\n",
            "train loss:0.09818945489391266\n",
            "train loss:0.022731049477424516\n",
            "train loss:0.08053588788425506\n",
            "train loss:0.09704238342591384\n",
            "train loss:0.05111729796501559\n",
            "train loss:0.051602934184447474\n",
            "train loss:0.021540240215748387\n",
            "train loss:0.024780657555801437\n",
            "train loss:0.02792915781746632\n",
            "train loss:0.03546716990276521\n",
            "train loss:0.05486092514913974\n",
            "train loss:0.03265631522614003\n",
            "train loss:0.1280977585123394\n",
            "train loss:0.040732365708508675\n",
            "train loss:0.03295164233678576\n",
            "train loss:0.03860196560880446\n",
            "train loss:0.06603937557912593\n",
            "train loss:0.10896691817220772\n",
            "train loss:0.025737339169010654\n",
            "train loss:0.027670129328413666\n",
            "train loss:0.12450426628310925\n",
            "train loss:0.04941878008324546\n",
            "train loss:0.019591510566495243\n",
            "train loss:0.03844563246558867\n",
            "train loss:0.0054092397256603085\n",
            "train loss:0.05819265851700812\n",
            "train loss:0.06610893203574854\n",
            "train loss:0.054944526099144166\n",
            "train loss:0.024979116196551535\n",
            "train loss:0.10263139311829537\n",
            "train loss:0.11998755546517764\n",
            "train loss:0.11446907597457806\n",
            "train loss:0.020435929209391804\n",
            "train loss:0.07232112191698639\n",
            "train loss:0.02849057540945551\n",
            "train loss:0.02411322898281752\n",
            "train loss:0.05968680412254665\n",
            "train loss:0.01818154233668564\n",
            "train loss:0.045575432679598016\n",
            "train loss:0.0884474189973482\n",
            "train loss:0.0588640136864678\n",
            "train loss:0.023884864153809682\n",
            "train loss:0.034230410359564305\n",
            "train loss:0.058258765078195024\n",
            "train loss:0.044547153661583155\n",
            "train loss:0.08968204047248643\n",
            "train loss:0.013367540583655644\n",
            "train loss:0.0680354462748269\n",
            "train loss:0.03419006632911662\n",
            "train loss:0.06099050812058417\n",
            "train loss:0.04402033000832029\n",
            "train loss:0.024884801006009614\n",
            "train loss:0.055879693422599984\n",
            "train loss:0.049135698555486045\n",
            "train loss:0.06855387633745101\n",
            "train loss:0.0353623659653773\n",
            "train loss:0.023179642462142706\n",
            "train loss:0.05067461550114466\n",
            "train loss:0.03507796928598194\n",
            "train loss:0.027475173925070467\n",
            "train loss:0.024689303726331543\n",
            "train loss:0.03667003888185833\n",
            "train loss:0.02389018407053405\n",
            "train loss:0.04387563036563817\n",
            "train loss:0.029367741712034916\n",
            "train loss:0.0871311930500804\n",
            "train loss:0.09021942670965646\n",
            "train loss:0.030811181186862747\n",
            "train loss:0.028954974596767272\n",
            "train loss:0.027721772761303297\n",
            "train loss:0.028359757457619664\n",
            "=== epoch:11, train acc:0.981, test acc:0.964 ===\n",
            "train loss:0.035788965177051234\n",
            "train loss:0.014489951863227865\n",
            "train loss:0.03974246180299609\n",
            "train loss:0.02274221054360035\n",
            "train loss:0.05291320460097687\n",
            "train loss:0.03063546488831129\n",
            "train loss:0.02351127416190857\n",
            "train loss:0.10614523402730364\n",
            "train loss:0.054348709308753136\n",
            "train loss:0.02749561579684131\n",
            "train loss:0.06483092375193591\n",
            "train loss:0.02942906917322857\n",
            "train loss:0.03233809516737208\n",
            "train loss:0.024242283567715187\n",
            "train loss:0.09592962458831046\n",
            "train loss:0.04532477387963135\n",
            "train loss:0.03578475000280275\n",
            "train loss:0.04834015912688606\n",
            "train loss:0.07566284448162636\n",
            "train loss:0.08408817955543865\n",
            "train loss:0.013372156038038685\n",
            "train loss:0.04628867006176782\n",
            "train loss:0.033373831248884066\n",
            "train loss:0.04654197111644597\n",
            "train loss:0.032958047861828085\n",
            "train loss:0.037795266170673227\n",
            "train loss:0.09555736991042911\n",
            "train loss:0.03999073150504358\n",
            "train loss:0.07065306507838391\n",
            "train loss:0.07776959207520534\n",
            "train loss:0.024903066698864697\n",
            "train loss:0.08802803291270557\n",
            "train loss:0.03823544388243814\n",
            "train loss:0.05140388301740062\n",
            "train loss:0.08828009694409505\n",
            "train loss:0.04278081138627625\n",
            "train loss:0.04127210571168021\n",
            "train loss:0.028014836494260863\n",
            "train loss:0.04466547194072402\n",
            "train loss:0.0533817986463837\n",
            "train loss:0.06416245073964214\n",
            "train loss:0.0329456006695025\n",
            "train loss:0.011150979003734842\n",
            "train loss:0.027105786510365725\n",
            "train loss:0.03133252207114563\n",
            "train loss:0.0399889345281962\n",
            "train loss:0.022422605132840983\n",
            "train loss:0.039296480944862024\n",
            "train loss:0.06525833644128674\n",
            "train loss:0.04858454735977584\n",
            "train loss:0.05426378398916513\n",
            "train loss:0.026646223170497044\n",
            "train loss:0.027434853350337886\n",
            "train loss:0.028608170216675638\n",
            "train loss:0.047250741453063565\n",
            "train loss:0.030613440075566257\n",
            "train loss:0.041630652336463445\n",
            "train loss:0.06497028036765326\n",
            "train loss:0.02205764247985794\n",
            "train loss:0.02568023262631661\n",
            "train loss:0.030378614405578624\n",
            "train loss:0.06910492343272798\n",
            "train loss:0.010327982464073838\n",
            "train loss:0.02608118465043066\n",
            "train loss:0.06064695131403985\n",
            "train loss:0.02812827350582143\n",
            "train loss:0.07704654520539131\n",
            "train loss:0.030080309735268474\n",
            "train loss:0.06752730248785604\n",
            "train loss:0.032296402199755075\n",
            "train loss:0.02107452366961003\n",
            "train loss:0.05711886293394938\n",
            "train loss:0.0181502487065734\n",
            "train loss:0.014160176149758208\n",
            "train loss:0.047112010114630266\n",
            "train loss:0.04375563747612668\n",
            "train loss:0.0185651119325977\n",
            "train loss:0.03548446458561268\n",
            "train loss:0.0674934365478107\n",
            "train loss:0.10326389909065117\n",
            "train loss:0.06737334146666338\n",
            "train loss:0.01829286915934916\n",
            "train loss:0.07579866657295391\n",
            "train loss:0.04684726072158561\n",
            "train loss:0.018703388506581032\n",
            "train loss:0.045695470506834236\n",
            "train loss:0.012829563759840483\n",
            "train loss:0.029523633705114734\n",
            "train loss:0.08803632518179834\n",
            "train loss:0.06618758769343139\n",
            "train loss:0.010586419644663687\n",
            "train loss:0.05397832124009368\n",
            "train loss:0.0452498189422776\n",
            "train loss:0.02206451880155207\n",
            "train loss:0.015871571761503347\n",
            "train loss:0.04055125651690109\n",
            "train loss:0.020701110317365386\n",
            "train loss:0.010392439715028248\n",
            "train loss:0.07024062063833895\n",
            "train loss:0.01745005467456078\n",
            "=== epoch:12, train acc:0.986, test acc:0.968 ===\n",
            "train loss:0.04009129654989772\n",
            "train loss:0.04455147365737269\n",
            "train loss:0.020285947970630113\n",
            "train loss:0.03086752081642398\n",
            "train loss:0.08627233848537066\n",
            "train loss:0.037525276780193434\n",
            "train loss:0.07948737347609428\n",
            "train loss:0.06119941283621918\n",
            "train loss:0.11125982898292351\n",
            "train loss:0.019366793726967414\n",
            "train loss:0.04842667598958198\n",
            "train loss:0.04283086830994422\n",
            "train loss:0.025262383573213806\n",
            "train loss:0.033711439824596984\n",
            "train loss:0.04401954775322164\n",
            "train loss:0.014538560628041748\n",
            "train loss:0.02261392314810426\n",
            "train loss:0.013907259441701873\n",
            "train loss:0.04808076909512133\n",
            "train loss:0.06796468527515785\n",
            "train loss:0.057523486131156515\n",
            "train loss:0.023235798723996857\n",
            "train loss:0.017823139457678674\n",
            "train loss:0.013805134320865962\n",
            "train loss:0.018780307299998182\n",
            "train loss:0.011671050510024185\n",
            "train loss:0.020208959561096463\n",
            "train loss:0.021377842150011293\n",
            "train loss:0.024087479770033105\n",
            "train loss:0.022740061118341683\n",
            "train loss:0.039616305190325204\n",
            "train loss:0.017917825121955964\n",
            "train loss:0.06547351741065463\n",
            "train loss:0.03257878000211877\n",
            "train loss:0.03640688634381097\n",
            "train loss:0.07871579425330458\n",
            "train loss:0.021062280818654137\n",
            "train loss:0.03309580946795498\n",
            "train loss:0.06807798642110026\n",
            "train loss:0.03753290617106213\n",
            "train loss:0.039972202145671626\n",
            "train loss:0.04247969421027205\n",
            "train loss:0.01763520835483514\n",
            "train loss:0.0174024096522565\n",
            "train loss:0.025781770549259765\n",
            "train loss:0.0378643705930894\n",
            "train loss:0.01714338292478307\n",
            "train loss:0.034340291773242146\n",
            "train loss:0.023312991437314235\n",
            "train loss:0.06996321409425915\n",
            "train loss:0.07622727093267102\n",
            "train loss:0.045731607647075796\n",
            "train loss:0.030849214221289833\n",
            "train loss:0.012708074346992934\n",
            "train loss:0.023306533628126777\n",
            "train loss:0.043052311008012795\n",
            "train loss:0.06155624963408453\n",
            "train loss:0.045394857053025615\n",
            "train loss:0.06394470156652748\n",
            "train loss:0.04541763272686507\n",
            "train loss:0.05056853018160641\n",
            "train loss:0.014772967671037576\n",
            "train loss:0.04183099132823243\n",
            "train loss:0.006821234492877264\n",
            "train loss:0.02059360643440571\n",
            "train loss:0.024606864926275805\n",
            "train loss:0.03218692525384975\n",
            "train loss:0.09597188928258069\n",
            "train loss:0.06760582592776279\n",
            "train loss:0.05131537965703447\n",
            "train loss:0.016804208532194668\n",
            "train loss:0.024867093374814953\n",
            "train loss:0.021882787017000555\n",
            "train loss:0.13127316863091154\n",
            "train loss:0.04112873973587992\n",
            "train loss:0.012436413760613467\n",
            "train loss:0.02308445439831509\n",
            "train loss:0.0436744179411279\n",
            "train loss:0.032397892861356865\n",
            "train loss:0.013600018983200613\n",
            "train loss:0.02354018346383833\n",
            "train loss:0.012016935155825139\n",
            "train loss:0.020146577758531597\n",
            "train loss:0.010233457989157814\n",
            "train loss:0.02153200879135991\n",
            "train loss:0.05854985341757139\n",
            "train loss:0.025620105113427848\n",
            "train loss:0.007675332031248652\n",
            "train loss:0.024344554987072668\n",
            "train loss:0.01948138545075454\n",
            "train loss:0.016410329434039358\n",
            "train loss:0.05945058233441925\n",
            "train loss:0.053208688693491005\n",
            "train loss:0.03274383304002184\n",
            "train loss:0.019621419139638087\n",
            "train loss:0.014169622030240445\n",
            "train loss:0.015394802513729067\n",
            "train loss:0.017185838868614033\n",
            "train loss:0.04515157710824856\n",
            "train loss:0.02719211187221265\n",
            "=== epoch:13, train acc:0.989, test acc:0.968 ===\n",
            "train loss:0.02480197063555416\n",
            "train loss:0.02943579021022126\n",
            "train loss:0.04272582051926439\n",
            "train loss:0.025302244528792484\n",
            "train loss:0.02759613014598444\n",
            "train loss:0.03471619189045759\n",
            "train loss:0.03610452831681373\n",
            "train loss:0.03346792148634898\n",
            "train loss:0.012471899956132591\n",
            "train loss:0.029611394368813188\n",
            "train loss:0.029736145860713447\n",
            "train loss:0.05288644076495569\n",
            "train loss:0.01267164105570665\n",
            "train loss:0.017770318572737333\n",
            "train loss:0.02655873673987428\n",
            "train loss:0.024328825497565508\n",
            "train loss:0.03660975425958423\n",
            "train loss:0.12313242062214283\n",
            "train loss:0.06433932482951789\n",
            "train loss:0.028192211972212375\n",
            "train loss:0.03858417604484321\n",
            "train loss:0.01843399345590351\n",
            "train loss:0.03817337783433663\n",
            "train loss:0.05219932276239481\n",
            "train loss:0.09554873321149902\n",
            "train loss:0.015653642996049752\n",
            "train loss:0.020300423005106535\n",
            "train loss:0.009809374779268031\n",
            "train loss:0.036632859452702685\n",
            "train loss:0.03318413640121479\n",
            "train loss:0.03153605292803821\n",
            "train loss:0.012947306721557455\n",
            "train loss:0.029293730115611066\n",
            "train loss:0.015374232251365662\n",
            "train loss:0.010064464224979745\n",
            "train loss:0.03643708047033221\n",
            "train loss:0.03117059020161068\n",
            "train loss:0.03029671612836398\n",
            "train loss:0.021098346018737918\n",
            "train loss:0.08404989066283855\n",
            "train loss:0.025277995296012726\n",
            "train loss:0.030032003460645475\n",
            "train loss:0.018144247283537092\n",
            "train loss:0.009739958035786412\n",
            "train loss:0.10552138741204421\n",
            "train loss:0.05329899968351274\n",
            "train loss:0.058236551183205816\n",
            "train loss:0.00808786883938922\n",
            "train loss:0.005966849375488745\n",
            "train loss:0.09922044312477973\n",
            "train loss:0.058133471889713614\n",
            "train loss:0.028779851683981936\n",
            "train loss:0.04167740331603001\n",
            "train loss:0.016123890688152988\n",
            "train loss:0.02717642660928965\n",
            "train loss:0.011413295293436283\n",
            "train loss:0.1327233838847468\n",
            "train loss:0.11066722026969851\n",
            "train loss:0.0809688461726352\n",
            "train loss:0.03370839199583799\n",
            "train loss:0.029436790133384853\n",
            "train loss:0.025737745438392932\n",
            "train loss:0.041301994321532504\n",
            "train loss:0.0181644749858942\n",
            "train loss:0.05082402121006169\n",
            "train loss:0.027112860899281027\n",
            "train loss:0.013230235905645014\n",
            "train loss:0.029618854741425876\n",
            "train loss:0.02233676431661989\n",
            "train loss:0.023278657480930557\n",
            "train loss:0.015424232577875127\n",
            "train loss:0.03192354230637705\n",
            "train loss:0.010573680167049616\n",
            "train loss:0.01742021492083387\n",
            "train loss:0.030720637097639437\n",
            "train loss:0.040289630287836296\n",
            "train loss:0.02175061535345981\n",
            "train loss:0.0702493832034721\n",
            "train loss:0.051293635185792\n",
            "train loss:0.02642094876636591\n",
            "train loss:0.03107390383885069\n",
            "train loss:0.007979737236868614\n",
            "train loss:0.03782685440318669\n",
            "train loss:0.030352654954238657\n",
            "train loss:0.039823644915926895\n",
            "train loss:0.027529092704987334\n",
            "train loss:0.022743448717849165\n",
            "train loss:0.03579248644431937\n",
            "train loss:0.009264677083596837\n",
            "train loss:0.01641491334064837\n",
            "train loss:0.11396366059935936\n",
            "train loss:0.010242924072419113\n",
            "train loss:0.01796897751452337\n",
            "train loss:0.02513578987697756\n",
            "train loss:0.011687341107488536\n",
            "train loss:0.03418261951217497\n",
            "train loss:0.05648937803584387\n",
            "train loss:0.012195402139708293\n",
            "train loss:0.011824587567904755\n",
            "train loss:0.020695308674991534\n",
            "=== epoch:14, train acc:0.992, test acc:0.969 ===\n",
            "train loss:0.014900476605377102\n",
            "train loss:0.011234081048263051\n",
            "train loss:0.022492711254711594\n",
            "train loss:0.01612755768394246\n",
            "train loss:0.023842380434114855\n",
            "train loss:0.05287624873372589\n",
            "train loss:0.03325013175565072\n",
            "train loss:0.03044575968055898\n",
            "train loss:0.012412316785688146\n",
            "train loss:0.0251092985196624\n",
            "train loss:0.01175177591268283\n",
            "train loss:0.03260834161345297\n",
            "train loss:0.014965984493658786\n",
            "train loss:0.01727325230866451\n",
            "train loss:0.03503177751571003\n",
            "train loss:0.009812995203514854\n",
            "train loss:0.012506960330061925\n",
            "train loss:0.019965483224064277\n",
            "train loss:0.016780408305048945\n",
            "train loss:0.007554151374353097\n",
            "train loss:0.04016664079164292\n",
            "train loss:0.045631848126544264\n",
            "train loss:0.03201370656859\n",
            "train loss:0.018442044487136735\n",
            "train loss:0.020978265598462786\n",
            "train loss:0.009265950393233744\n",
            "train loss:0.025725522770835955\n",
            "train loss:0.049519522858088354\n",
            "train loss:0.02088518383766277\n",
            "train loss:0.0451775795807416\n",
            "train loss:0.02031396139949743\n",
            "train loss:0.031017832891789915\n",
            "train loss:0.022997771958319625\n",
            "train loss:0.0427485983251935\n",
            "train loss:0.0666274149405302\n",
            "train loss:0.0190486976560818\n",
            "train loss:0.07124960276654382\n",
            "train loss:0.020299900549249683\n",
            "train loss:0.04034926355261964\n",
            "train loss:0.02144249169238482\n",
            "train loss:0.012197312008159971\n",
            "train loss:0.019422401553020843\n",
            "train loss:0.014832939098433478\n",
            "train loss:0.048472643254051206\n",
            "train loss:0.00843881057161827\n",
            "train loss:0.035310712822098994\n",
            "train loss:0.018898200862879765\n",
            "train loss:0.009682469094044995\n",
            "train loss:0.01805029996328878\n",
            "train loss:0.04583434802065658\n",
            "train loss:0.03272647966376788\n",
            "train loss:0.09142440988586137\n",
            "train loss:0.009781537023890704\n",
            "train loss:0.020162317405043075\n",
            "train loss:0.024207626451562873\n",
            "train loss:0.03377088778436287\n",
            "train loss:0.03564960939556389\n",
            "train loss:0.011650665202026414\n",
            "train loss:0.00802791059545569\n",
            "train loss:0.07690185708315962\n",
            "train loss:0.025952724724545847\n",
            "train loss:0.019191962507414496\n",
            "train loss:0.014103618024233633\n",
            "train loss:0.024046498494511886\n",
            "train loss:0.02007468288616924\n",
            "train loss:0.0266057016056362\n",
            "train loss:0.03160577039077184\n",
            "train loss:0.03408269969989701\n",
            "train loss:0.0161513040400138\n",
            "train loss:0.012135043150821817\n",
            "train loss:0.013357344623790174\n",
            "train loss:0.027018140458694186\n",
            "train loss:0.018621764974045565\n",
            "train loss:0.0769746222807827\n",
            "train loss:0.03448996327432983\n",
            "train loss:0.02737859129771067\n",
            "train loss:0.027781711229536748\n",
            "train loss:0.01583120969487121\n",
            "train loss:0.012227209588017733\n",
            "train loss:0.005765241435592677\n",
            "train loss:0.013962317229079679\n",
            "train loss:0.08145192271421087\n",
            "train loss:0.034347536963818655\n",
            "train loss:0.015533656830347459\n",
            "train loss:0.016452618661541404\n",
            "train loss:0.009582045751842632\n",
            "train loss:0.015752812064936283\n",
            "train loss:0.019961358066623892\n",
            "train loss:0.01178819256920103\n",
            "train loss:0.010348290524027515\n",
            "train loss:0.033443615626396625\n",
            "train loss:0.020701563089822156\n",
            "train loss:0.051886830376108826\n",
            "train loss:0.00801770860722993\n",
            "train loss:0.08233920808308241\n",
            "train loss:0.11537098718973139\n",
            "train loss:0.022096242716672612\n",
            "train loss:0.028721139797024894\n",
            "train loss:0.0035239481511853054\n",
            "train loss:0.015175354505601394\n",
            "=== epoch:15, train acc:0.99, test acc:0.972 ===\n",
            "train loss:0.008725867096817186\n",
            "train loss:0.03722582312794129\n",
            "train loss:0.028213230435060254\n",
            "train loss:0.010880394493545735\n",
            "train loss:0.036185952455486926\n",
            "train loss:0.018110928070561985\n",
            "train loss:0.0066793808000209206\n",
            "train loss:0.009314024305514949\n",
            "train loss:0.014516044218636585\n",
            "train loss:0.030947103470099546\n",
            "train loss:0.024307094363536588\n",
            "train loss:0.017572645176705544\n",
            "train loss:0.013958466356438241\n",
            "train loss:0.01222176031431059\n",
            "train loss:0.015658142699369486\n",
            "train loss:0.03750512083465424\n",
            "train loss:0.06283329313498706\n",
            "train loss:0.09820493343288322\n",
            "train loss:0.021122425801558778\n",
            "train loss:0.021135125570667577\n",
            "train loss:0.018673844866672812\n",
            "train loss:0.015235149716615284\n",
            "train loss:0.024826331948562257\n",
            "train loss:0.017042062220633498\n",
            "train loss:0.011448825197467642\n",
            "train loss:0.018589649017168364\n",
            "train loss:0.019514721866385313\n",
            "train loss:0.017561146446211776\n",
            "train loss:0.011851097005839105\n",
            "train loss:0.0733707603075033\n",
            "train loss:0.014072042743469834\n",
            "train loss:0.025124399844069906\n",
            "train loss:0.03262489062844378\n",
            "train loss:0.024931234249222934\n",
            "train loss:0.041859191859653117\n",
            "train loss:0.013621623593321024\n",
            "train loss:0.03340213940002616\n",
            "train loss:0.02261650169466787\n",
            "train loss:0.022300427249139564\n",
            "train loss:0.012339680793541433\n",
            "train loss:0.02037279699033173\n",
            "train loss:0.016000606566537134\n",
            "train loss:0.09159551226837065\n",
            "train loss:0.010625054094491175\n",
            "train loss:0.013792866723810334\n",
            "train loss:0.03107020589016278\n",
            "train loss:0.02169620988075072\n",
            "train loss:0.03927743172502756\n",
            "train loss:0.019516916429747174\n",
            "train loss:0.022202553957129036\n",
            "train loss:0.04995321076990469\n",
            "train loss:0.013962825578257863\n",
            "train loss:0.017065826211703824\n",
            "train loss:0.006268541056475535\n",
            "train loss:0.03505066366566901\n",
            "train loss:0.020439331504435512\n",
            "train loss:0.014345802840003155\n",
            "train loss:0.015373527828275774\n",
            "train loss:0.038976387200646105\n",
            "train loss:0.024905241291297855\n",
            "train loss:0.024425791175676222\n",
            "train loss:0.006225574811198434\n",
            "train loss:0.0091114115882138\n",
            "train loss:0.02302229711216816\n",
            "train loss:0.012921814997267522\n",
            "train loss:0.023415862881914364\n",
            "train loss:0.04506853618359595\n",
            "train loss:0.029603831276289173\n",
            "train loss:0.009173498132988239\n",
            "train loss:0.03493241836375599\n",
            "train loss:0.00703336077289698\n",
            "train loss:0.024009992987169895\n",
            "train loss:0.013450673391415001\n",
            "train loss:0.04881638943488692\n",
            "train loss:0.03124095046528471\n",
            "train loss:0.017768145474167493\n",
            "train loss:0.013369242366862736\n",
            "train loss:0.021461898634840618\n",
            "train loss:0.029216346056608517\n",
            "train loss:0.03750551063343056\n",
            "train loss:0.008884288083360937\n",
            "train loss:0.010144371979766904\n",
            "train loss:0.023309537123669392\n",
            "train loss:0.014636083453242734\n",
            "train loss:0.005947257635275248\n",
            "train loss:0.007256047870346287\n",
            "train loss:0.015554353950522388\n",
            "train loss:0.0059132208519201345\n",
            "train loss:0.004993636338092003\n",
            "train loss:0.011448916450448399\n",
            "train loss:0.01774132332518271\n",
            "train loss:0.02278090931076615\n",
            "train loss:0.013717251973213072\n",
            "train loss:0.016805070584139766\n",
            "train loss:0.014624993417833505\n",
            "train loss:0.01374481975542775\n",
            "train loss:0.08028574450919602\n",
            "train loss:0.0060759276129487986\n",
            "train loss:0.01612497702205374\n",
            "train loss:0.026951877922015224\n",
            "=== epoch:16, train acc:0.995, test acc:0.969 ===\n",
            "train loss:0.0024579229142737266\n",
            "train loss:0.029320258613144457\n",
            "train loss:0.009006297658205637\n",
            "train loss:0.007931474441922891\n",
            "train loss:0.007867765214215639\n",
            "train loss:0.03980638614105965\n",
            "train loss:0.006080046691043004\n",
            "train loss:0.008934583802074713\n",
            "train loss:0.0333575482657005\n",
            "train loss:0.01659294688193106\n",
            "train loss:0.010266121523554073\n",
            "train loss:0.02469417145885449\n",
            "train loss:0.03824214822188028\n",
            "train loss:0.024675265540643854\n",
            "train loss:0.014905312184852394\n",
            "train loss:0.029217458114116926\n",
            "train loss:0.056612169017983084\n",
            "train loss:0.008966209507725842\n",
            "train loss:0.03454903460577582\n",
            "train loss:0.019272096900975758\n",
            "train loss:0.017008111100688714\n",
            "train loss:0.015438632444390745\n",
            "train loss:0.008127322309451034\n",
            "train loss:0.015753947796903603\n",
            "train loss:0.0146413982134697\n",
            "train loss:0.010243117270160503\n",
            "train loss:0.0072160494812844485\n",
            "train loss:0.03077521441146095\n",
            "train loss:0.02042688741784143\n",
            "train loss:0.02794870267540849\n",
            "train loss:0.04363026839150026\n",
            "train loss:0.007144784901201513\n",
            "train loss:0.01683654674527159\n",
            "train loss:0.018727255195627398\n",
            "train loss:0.013001766796228935\n",
            "train loss:0.012380540719439526\n",
            "train loss:0.010513561299935454\n",
            "train loss:0.006661133366284963\n",
            "train loss:0.014221245612194898\n",
            "train loss:0.015995273284946262\n",
            "train loss:0.013202900220329447\n",
            "train loss:0.01032689664172568\n",
            "train loss:0.013702029717754208\n",
            "train loss:0.02862204919330559\n",
            "train loss:0.017899116732331456\n",
            "train loss:0.009735267925025761\n",
            "train loss:0.013225925517723421\n",
            "train loss:0.012580351630200411\n",
            "train loss:0.06801424727735231\n",
            "train loss:0.014889724554606685\n",
            "train loss:0.007677258961942784\n",
            "train loss:0.02123300286985295\n",
            "train loss:0.031096498153686904\n",
            "train loss:0.005938987126907642\n",
            "train loss:0.012384446538765073\n",
            "train loss:0.0287298526882223\n",
            "train loss:0.018423397099303834\n",
            "train loss:0.009277337251809174\n",
            "train loss:0.005950270799424876\n",
            "train loss:0.012200277308010415\n",
            "train loss:0.02357367065043793\n",
            "train loss:0.006285856760235313\n",
            "train loss:0.016886524745749722\n",
            "train loss:0.035813118451192893\n",
            "train loss:0.04687398229053987\n",
            "train loss:0.0633797673664443\n",
            "train loss:0.013755036479391533\n",
            "train loss:0.01976911909066281\n",
            "train loss:0.019254439099631215\n",
            "train loss:0.017689001271674304\n",
            "train loss:0.005071207712777027\n",
            "train loss:0.01373679255830631\n",
            "train loss:0.03303239830773382\n",
            "train loss:0.0305893019289073\n",
            "train loss:0.010103352556588029\n",
            "train loss:0.020401721028296547\n",
            "train loss:0.027026897087779497\n",
            "train loss:0.022129522990487413\n",
            "train loss:0.006123534562919908\n",
            "train loss:0.01729494895608174\n",
            "train loss:0.010033521977215227\n",
            "train loss:0.028612121916036362\n",
            "train loss:0.005441785586798935\n",
            "train loss:0.015390738767438144\n",
            "train loss:0.026375797357576825\n",
            "train loss:0.007133944915466478\n",
            "train loss:0.01700855688037632\n",
            "train loss:0.006102003815759748\n",
            "train loss:0.010050327057063255\n",
            "train loss:0.014905248081207434\n",
            "train loss:0.03326567663634863\n",
            "train loss:0.007238323212737847\n",
            "train loss:0.02864243329498701\n",
            "train loss:0.014345308414550846\n",
            "train loss:0.012474606646200195\n",
            "train loss:0.023154731710367894\n",
            "train loss:0.01565227014615023\n",
            "train loss:0.016880900160147493\n",
            "train loss:0.006649374190928857\n",
            "train loss:0.01140156120294775\n",
            "=== epoch:17, train acc:0.995, test acc:0.969 ===\n",
            "train loss:0.004790515468131707\n",
            "train loss:0.018582664589684744\n",
            "train loss:0.0032142925543476486\n",
            "train loss:0.01237456240098512\n",
            "train loss:0.004168552069850216\n",
            "train loss:0.00474207349514775\n",
            "train loss:0.01291861881554235\n",
            "train loss:0.030901648334850297\n",
            "train loss:0.017591133455866305\n",
            "train loss:0.010596494043592085\n",
            "train loss:0.007148254636895373\n",
            "train loss:0.01082477815446066\n",
            "train loss:0.0382000784042272\n",
            "train loss:0.020572412761286046\n",
            "train loss:0.007305699920660831\n",
            "train loss:0.019934784773967235\n",
            "train loss:0.00330987663803258\n",
            "train loss:0.02986126283852821\n",
            "train loss:0.003548898377000293\n",
            "train loss:0.01221808970835321\n",
            "train loss:0.014802274971429863\n",
            "train loss:0.03350561785816958\n",
            "train loss:0.010266244588157529\n",
            "train loss:0.01069297506578442\n",
            "train loss:0.030624445099117565\n",
            "train loss:0.020861690042920204\n",
            "train loss:0.01773800146282269\n",
            "train loss:0.028941033812328012\n",
            "train loss:0.010025485726041623\n",
            "train loss:0.012624188331800101\n",
            "train loss:0.010836092747653885\n",
            "train loss:0.015390811072127366\n",
            "train loss:0.011356600068547398\n",
            "train loss:0.012412990799160513\n",
            "train loss:0.009249204349920919\n",
            "train loss:0.008792932054966302\n",
            "train loss:0.025814912667164312\n",
            "train loss:0.007631962194413439\n",
            "train loss:0.05287750374798409\n",
            "train loss:0.013854587823539086\n",
            "train loss:0.005165494822458558\n",
            "train loss:0.00493441620156213\n",
            "train loss:0.008761999732038167\n",
            "train loss:0.008348750443893152\n",
            "train loss:0.012316013853102537\n",
            "train loss:0.00879460783734824\n",
            "train loss:0.008202472098741231\n",
            "train loss:0.005170670718106011\n",
            "train loss:0.01610181452746306\n",
            "train loss:0.026833314381363636\n",
            "train loss:0.012822421309333993\n",
            "train loss:0.014298128518449416\n",
            "train loss:0.009498229218555036\n",
            "train loss:0.009149810278115497\n",
            "train loss:0.014803379960418776\n",
            "train loss:0.015072348084537128\n",
            "train loss:0.002514004778421631\n",
            "train loss:0.022231175768114758\n",
            "train loss:0.00577491795972176\n",
            "train loss:0.024894706787548862\n",
            "train loss:0.006701094232671858\n",
            "train loss:0.006418672134897465\n",
            "train loss:0.020334204410690303\n",
            "train loss:0.004936207775775169\n",
            "train loss:0.003450016954517999\n",
            "train loss:0.01915162928119356\n",
            "train loss:0.006529140653979819\n",
            "train loss:0.013896165451099358\n",
            "train loss:0.008227005550505147\n",
            "train loss:0.00775603788557365\n",
            "train loss:0.009890504381169394\n",
            "train loss:0.009076569323021342\n",
            "train loss:0.005090043753665722\n",
            "train loss:0.0141305083207107\n",
            "train loss:0.012040649764921661\n",
            "train loss:0.006394629793696601\n",
            "train loss:0.005335133287354005\n",
            "train loss:0.00792791257155628\n",
            "train loss:0.023613274596354573\n",
            "train loss:0.012872393435906094\n",
            "train loss:0.00957463729808866\n",
            "train loss:0.01916791571868457\n",
            "train loss:0.012281232818346581\n",
            "train loss:0.015725920662201423\n",
            "train loss:0.015957963613498584\n",
            "train loss:0.018059423324865632\n",
            "train loss:0.010083866180394392\n",
            "train loss:0.00811040317637239\n",
            "train loss:0.016613814224152514\n",
            "train loss:0.005289789317638438\n",
            "train loss:0.011288403933136393\n",
            "train loss:0.006280057710389086\n",
            "train loss:0.008009779774859381\n",
            "train loss:0.023371582717947138\n",
            "train loss:0.004934804124691253\n",
            "train loss:0.04319917198173326\n",
            "train loss:0.0024547966204800007\n",
            "train loss:0.005956171937281197\n",
            "train loss:0.052125515849682016\n",
            "train loss:0.005415723377533009\n",
            "=== epoch:18, train acc:0.995, test acc:0.969 ===\n",
            "train loss:0.015777041588899265\n",
            "train loss:0.017303426179561814\n",
            "train loss:0.012826999164414425\n",
            "train loss:0.004861471935535408\n",
            "train loss:0.004617333668829044\n",
            "train loss:0.005129577776514178\n",
            "train loss:0.012415972080812682\n",
            "train loss:0.004054896762738685\n",
            "train loss:0.01745639958279088\n",
            "train loss:0.028381129711823373\n",
            "train loss:0.006529228895589033\n",
            "train loss:0.02673248034716677\n",
            "train loss:0.007850844806648842\n",
            "train loss:0.01452977095075924\n",
            "train loss:0.009915679950220862\n",
            "train loss:0.007087415362731338\n",
            "train loss:0.009458276575405194\n",
            "train loss:0.0025151582486480977\n",
            "train loss:0.010066899953775193\n",
            "train loss:0.015706713316123052\n",
            "train loss:0.008304021698020943\n",
            "train loss:0.012599682898431108\n",
            "train loss:0.008208349106607447\n",
            "train loss:0.005799657757949835\n",
            "train loss:0.00489973771720182\n",
            "train loss:0.007450299361798641\n",
            "train loss:0.00841262392078939\n",
            "train loss:0.007314298517758223\n",
            "train loss:0.008663715803885806\n",
            "train loss:0.011293313677766457\n",
            "train loss:0.007114736185410228\n",
            "train loss:0.014154147756362088\n",
            "train loss:0.005498052631430212\n",
            "train loss:0.011241836615948818\n",
            "train loss:0.019779376247377477\n",
            "train loss:0.017250751980152995\n",
            "train loss:0.002633689840264819\n",
            "train loss:0.006115734397452679\n",
            "train loss:0.01153001135705866\n",
            "train loss:0.023726943233950925\n",
            "train loss:0.0042689923071145535\n",
            "train loss:0.008907524967027155\n",
            "train loss:0.026122787954225127\n",
            "train loss:0.009378121552867411\n",
            "train loss:0.012143311383996347\n",
            "train loss:0.04013142294774255\n",
            "train loss:0.00775303459737655\n",
            "train loss:0.010916788768034607\n",
            "train loss:0.026609142865999316\n",
            "train loss:0.02260269837839926\n",
            "train loss:0.023336659709067856\n",
            "train loss:0.01224606380896096\n",
            "train loss:0.0034778762158643884\n",
            "train loss:0.006840897709359809\n",
            "train loss:0.008356068870923489\n",
            "train loss:0.012566050428542362\n",
            "train loss:0.005864855736608696\n",
            "train loss:0.009486657079058528\n",
            "train loss:0.006747333616464412\n",
            "train loss:0.004440799096876335\n",
            "train loss:0.051872342211706626\n",
            "train loss:0.014381111502211538\n",
            "train loss:0.0057415583417975615\n",
            "train loss:0.013421476011493603\n",
            "train loss:0.005865614632005321\n",
            "train loss:0.05044482302700163\n",
            "train loss:0.010763397934025644\n",
            "train loss:0.02748135298184346\n",
            "train loss:0.014294485043120374\n",
            "train loss:0.009218333380290733\n",
            "train loss:0.023831112671891835\n",
            "train loss:0.012741762741998439\n",
            "train loss:0.00968880892305088\n",
            "train loss:0.010125470054850714\n",
            "train loss:0.014214968407881057\n",
            "train loss:0.006357762420785257\n",
            "train loss:0.005518183528240219\n",
            "train loss:0.0427879515447479\n",
            "train loss:0.005195771438870452\n",
            "train loss:0.0028200669444732353\n",
            "train loss:0.010980117833026413\n",
            "train loss:0.013896080256900924\n",
            "train loss:0.013131549280993374\n",
            "train loss:0.002167834148770365\n",
            "train loss:0.00978806061108267\n",
            "train loss:0.01245608708631968\n",
            "train loss:0.005998157590528444\n",
            "train loss:0.042940340543148506\n",
            "train loss:0.0047918659389308655\n",
            "train loss:0.005362833147489703\n",
            "train loss:0.008198398274180183\n",
            "train loss:0.011928538047057959\n",
            "train loss:0.0026346597147262484\n",
            "train loss:0.013804865002177646\n",
            "train loss:0.022426561574817393\n",
            "train loss:0.011688475804697783\n",
            "train loss:0.010693439421897743\n",
            "train loss:0.03212034915375522\n",
            "train loss:0.003252308148482392\n",
            "train loss:0.006258841100908059\n",
            "=== epoch:19, train acc:0.999, test acc:0.972 ===\n",
            "train loss:0.01026224383325304\n",
            "train loss:0.006984194794857385\n",
            "train loss:0.010562469037980004\n",
            "train loss:0.008959994216157393\n",
            "train loss:0.013559404063345515\n",
            "train loss:0.005805430131930906\n",
            "train loss:0.03236979256782578\n",
            "train loss:0.00951391658068105\n",
            "train loss:0.009707549306181777\n",
            "train loss:0.00662285999170412\n",
            "train loss:0.010860342560394953\n",
            "train loss:0.005082637641805701\n",
            "train loss:0.007879588968961938\n",
            "train loss:0.013766522736040662\n",
            "train loss:0.0038008400108226845\n",
            "train loss:0.007202795212869055\n",
            "train loss:0.009067812885580128\n",
            "train loss:0.0056254235433660215\n",
            "train loss:0.022769949730438707\n",
            "train loss:0.010004411207564259\n",
            "train loss:0.027503763586993316\n",
            "train loss:0.005606462079630595\n",
            "train loss:0.08357512260277085\n",
            "train loss:0.0077762828664030515\n",
            "train loss:0.026758966495460276\n",
            "train loss:0.009118200065698157\n",
            "train loss:0.006376551653898308\n",
            "train loss:0.007057933097418693\n",
            "train loss:0.0025763603533915623\n",
            "train loss:0.013930422173595698\n",
            "train loss:0.006801051157824838\n",
            "train loss:0.014344687257615752\n",
            "train loss:0.011200025853629869\n",
            "train loss:0.004573515635479249\n",
            "train loss:0.012378825792790813\n",
            "train loss:0.005055329464658844\n",
            "train loss:0.00988569685060362\n",
            "train loss:0.02557359352209849\n",
            "train loss:0.010117073497469981\n",
            "train loss:0.0037677296104708406\n",
            "train loss:0.012498074412220015\n",
            "train loss:0.012051447445948986\n",
            "train loss:0.007763156298274113\n",
            "train loss:0.005905753963439349\n",
            "train loss:0.018633169290105632\n",
            "train loss:0.01137762964018578\n",
            "train loss:0.012926470628818796\n",
            "train loss:0.022198377879057887\n",
            "train loss:0.010133766311870608\n",
            "train loss:0.005851454372960124\n",
            "train loss:0.013487743644810224\n",
            "train loss:0.009209350460548956\n",
            "train loss:0.008927927800509777\n",
            "train loss:0.004084249386302898\n",
            "train loss:0.02109143686666425\n",
            "train loss:0.019159039796831565\n",
            "train loss:0.004360620526070674\n",
            "train loss:0.010276170693555792\n",
            "train loss:0.01002334904945184\n",
            "train loss:0.0348009351008113\n",
            "train loss:0.11306140920217998\n",
            "train loss:0.01208221881699555\n",
            "train loss:0.048027682602922984\n",
            "train loss:0.005397099044447604\n",
            "train loss:0.021484190013869076\n",
            "train loss:0.009397800310897124\n",
            "train loss:0.013245682089912773\n",
            "train loss:0.017997476888656633\n",
            "train loss:0.02074937832518986\n",
            "train loss:0.01090765635997775\n",
            "train loss:0.030304917112413496\n",
            "train loss:0.006688570470823167\n",
            "train loss:0.003864886385758597\n",
            "train loss:0.008599244177934643\n",
            "train loss:0.00420790704746719\n",
            "train loss:0.007658591927435342\n",
            "train loss:0.006453934965758732\n",
            "train loss:0.025189715218370184\n",
            "train loss:0.011119276194535264\n",
            "train loss:0.01296361570996741\n",
            "train loss:0.007574542506229821\n",
            "train loss:0.014298812741428541\n",
            "train loss:0.004827303423642363\n",
            "train loss:0.00936935515832515\n",
            "train loss:0.010143345501369005\n",
            "train loss:0.022015770888349516\n",
            "train loss:0.005290939284307532\n",
            "train loss:0.007888789481137136\n",
            "train loss:0.011930355720884765\n",
            "train loss:0.011906300072642975\n",
            "train loss:0.006137839507264324\n",
            "train loss:0.0019208868604194565\n",
            "train loss:0.008227454366797816\n",
            "train loss:0.010492334699624304\n",
            "train loss:0.008199406250280343\n",
            "train loss:0.005174743992653757\n",
            "train loss:0.017218448527764562\n",
            "train loss:0.01675778677771181\n",
            "train loss:0.009786033503348645\n",
            "train loss:0.009282791418968378\n",
            "=== epoch:20, train acc:0.998, test acc:0.97 ===\n",
            "train loss:0.014272510224206164\n",
            "train loss:0.02603657175590889\n",
            "train loss:0.0031927094131499857\n",
            "train loss:0.010168171010337077\n",
            "train loss:0.007598105263604869\n",
            "train loss:0.008995727512725206\n",
            "train loss:0.008402191571631756\n",
            "train loss:0.014037689110303264\n",
            "train loss:0.00605572881413245\n",
            "train loss:0.01225913244571468\n",
            "train loss:0.003174938292930929\n",
            "train loss:0.009222072563283978\n",
            "train loss:0.01687638687630522\n",
            "train loss:0.004741117352783177\n",
            "train loss:0.004193002460042798\n",
            "train loss:0.003348458791735067\n",
            "train loss:0.008635794877924909\n",
            "train loss:0.005793526641252547\n",
            "train loss:0.005679104078411699\n",
            "train loss:0.010212920325228064\n",
            "train loss:0.009100051686069723\n",
            "train loss:0.00585499527850726\n",
            "train loss:0.003771446297212178\n",
            "train loss:0.013241220237159866\n",
            "train loss:0.0062494414207870915\n",
            "train loss:0.010892690350317424\n",
            "train loss:0.009868608868360005\n",
            "train loss:0.01593393464554281\n",
            "train loss:0.004626220902768514\n",
            "train loss:0.009601930552510607\n",
            "train loss:0.00997425557583207\n",
            "train loss:0.003950003467106548\n",
            "train loss:0.009971597320493653\n",
            "train loss:0.015210735287847872\n",
            "train loss:0.0023109141718842\n",
            "train loss:0.012324266929932883\n",
            "train loss:0.006710154348089528\n",
            "train loss:0.0018065585257404711\n",
            "train loss:0.008221625792070529\n",
            "train loss:0.004844124311601511\n",
            "train loss:0.007641753654793864\n",
            "train loss:0.004046739301867543\n",
            "train loss:0.010887025793087448\n",
            "train loss:0.005029316902145461\n",
            "train loss:0.011793374843314884\n",
            "train loss:0.014165576569743505\n",
            "train loss:0.009150194389533827\n",
            "train loss:0.003063874497428681\n",
            "train loss:0.004530396473623281\n",
            "train loss:0.016008970525010543\n",
            "train loss:0.003436023053910161\n",
            "train loss:0.009944838406916245\n",
            "train loss:0.01763053698721263\n",
            "train loss:0.01537634512107735\n",
            "train loss:0.009316267947496331\n",
            "train loss:0.0038339812661813575\n",
            "train loss:0.005052997269975099\n",
            "train loss:0.012102592742267826\n",
            "train loss:0.00582998265769976\n",
            "train loss:0.004792728189387189\n",
            "train loss:0.0033668098224452297\n",
            "train loss:0.0045616897166146165\n",
            "train loss:0.0050059237421649985\n",
            "train loss:0.024172658793095037\n",
            "train loss:0.008501955519278908\n",
            "train loss:0.005781438310817436\n",
            "train loss:0.009332755713727645\n",
            "train loss:0.009499285994031055\n",
            "train loss:0.002647099706393144\n",
            "train loss:0.00464210762102276\n",
            "train loss:0.0016662497141906518\n",
            "train loss:0.0029866781463291246\n",
            "train loss:0.011697878243616339\n",
            "train loss:0.03080707069917217\n",
            "train loss:0.006788682586870682\n",
            "train loss:0.010760097807374564\n",
            "train loss:0.003754666665028106\n",
            "train loss:0.009293608154095766\n",
            "train loss:0.009172971472791112\n",
            "train loss:0.0035211936876632737\n",
            "train loss:0.01651418092500552\n",
            "train loss:0.03981633821529941\n",
            "train loss:0.0015937227974288115\n",
            "train loss:0.0035107811275464575\n",
            "train loss:0.013916669242425133\n",
            "train loss:0.007175189694536769\n",
            "train loss:0.004399764264813173\n",
            "train loss:0.011939170741944288\n",
            "train loss:0.002871502126079931\n",
            "train loss:0.004802758041579241\n",
            "train loss:0.005806816996014248\n",
            "train loss:0.015266748465876206\n",
            "train loss:0.010106827894906984\n",
            "train loss:0.006671034284667363\n",
            "train loss:0.00201065877949188\n",
            "train loss:0.0037516053792418986\n",
            "train loss:0.002626568303734545\n",
            "train loss:0.004593553006956276\n",
            "train loss:0.013841936588719428\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9665\n"
          ]
        },
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: 'params.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m trainer.train()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_params\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved Network Parameters!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/DL-Excersize/notebooks/DL3_20241006/ch06/simple_convnet2.py:156\u001b[39m, in \u001b[36mSimpleConvNet.save_params\u001b[39m\u001b[34m(self, file_name)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.items():\n\u001b[32m    155\u001b[39m     params[key] = val\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    157\u001b[39m     pickle.dump(params, f)\n",
            "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'params.pkl'"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet2 import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test,t_test) = load_mnist(flatten=False)\n",
        "\n",
        "#    .\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), conv_param={'filter_num':30, 'filter_size':5, 'pad': 0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs, mini_batch_size=100, optimizer='Adam', optimizer_param={'lr':0.001}, evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# \n",
        "network.save_params('params.pkl')\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "#\n",
        "markers={'train':'o', 'test':'s'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label = 'train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
